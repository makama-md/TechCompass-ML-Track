## 12 Machine Learning (ML) Algorithms in 12 Weeks: Learning Outcomes

Weeks 1–2: Mathematical Foundations for Machine Learning 
Linear Algebra
	understand vectors, matrices, and matrix multiplication with respect to machine learning. 
	explore eigenvalues, eigenvectors
Statistics
	Learn basic concepts such as mean, variance, standard deviation, and histograms.
	Understand the probability distributions and basic data visualization. 
	Understand the basics of probability.
Calculus
	Understand the basics: derivatives, partial derivatives, and gradients. 
Weeks 3-12: Classical ML Algorithms
Week 3: Linear Regression
	Develop a mathematical understanding of linear relationships and predictive modeling.   
	Derive the normal equation for linear regression and implement it programmatically.   
Week 4: Logistic Regression
	Extend linear regression concepts to solve classification problems.   
	Understand the sigmoid function and use it in binary classification.
	Study different cost functions.  
Week 5: Decision Trees
	Understand how to form decision boundaries by entropy and information gain. 
	Implement simple decision trees to carry out classification tasks. 
	Learn about practical ways to avoid overfitting, such as pruning. 
Week 6: Random Forests
	Study ensemble methods and the concept of bagging in random forests.
	Understand feature importance and how randomness improves model robustness.  
	Analyze model performance with metrics like accuracy and confusion matrices.  
Week 7: Support Vector Machines (SVM)
	Learn the concept of hyperplanes and maximizing margins for classification tasks.  
	Introduce kernels for solving nonlinear classification problems.  
	Explore visualization of SVM decision boundaries.  
 Week 8: Naive Bayes
	Understand the principles of the Bayesian theorem and its assumptions of independence.  
	See how it is applied in real life in spam filtering and text classification.  
Week 9: KMeans Clustering
	Understand how KMeans does the job of clustering in an iteratively.  
	Visualize iterations of clusters and centroid updates.  
	Discuss some initialization challenges and elbow method for optimal K selection.  
Week 10: Principal Component Analysis (PCA)
	Simplify complex datasets using PCA.
	Understand how eigenvalues and eigenvectors are applied to detect principal components.  
	Apply PCA to a high dimensional dataset to reduce the dimensionality to 2D or 3D. 
Weeks 11–12: Advanced ML Algorithms
Neural Networks 
	Understand Perceptron, Activation functions and Architecture of Neural Networks.  
	Derive forward and backward propagation with simple examples.
	Explore gradient descent
	Optimization algorithms such as SGD, Adam, etc., and visualize how weights get updated.
